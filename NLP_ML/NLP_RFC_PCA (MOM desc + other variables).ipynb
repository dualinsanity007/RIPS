{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('nus_data_full_rand.csv', encoding= 'unicode_escape', index_col = False)\n",
    "\n",
    "labels = data['E_OCC']\n",
    "data['rand'] = pd.Series(np.random.uniform(0,1,len(labels.index)))\n",
    "data = data.sort_values(by=['rand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove Punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punc(text):\n",
    "    text_nopunc = \"\".join([char for char in text if char not in string.punctuation])  # discard all punctuation\n",
    "    return text_nopunc\n",
    "\n",
    "data['desc_clean'] = data['E_OCC_Desc'].apply(lambda x: remove_punc(x))\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Tokenize words\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenise(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['desc_tokenised'] = data['desc_clean'].apply(lambda x: tokenise(x.lower()))\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eugene\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Function to remove stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(lst):\n",
    "    text = [word for word in lst if word not in stopword]  # Remove all stopwords\n",
    "    return text\n",
    "\n",
    "data['desc_nostop'] = data['desc_tokenised'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(token):\n",
    "    text = [wn.lemmatize(word) for word in token]\n",
    "    return text\n",
    "\n",
    "data['desc_lemmatized'] = data['desc_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_counts = count_vect.fit_transform(data['E_OCC_Desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_array = pd.DataFrame(X_counts.toarray())\n",
    "# X_counts_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['TENH','SEX','RACE','ID_TYP','MARITAL_ST', 'E_EMPST', 'E_IND_Desc_LE', 'EDUC_N', 'AGE_G']\n",
    "\n",
    "train_features = pd.concat([data[cols], X_counts_array], axis = 1)\n",
    "# train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_features,labels,test_size=0.2,random_state=23, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_x_scaled = sc.fit_transform(train_x)\n",
    "test_x_scaled = sc.transform(test_x)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2062, 2460)\n",
      "(1649, 2460)\n",
      "(413, 2460)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "train_x_scaled = pca.fit_transform(train_x_scaled)\n",
    "test_x_scaled = pca.transform(test_x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1649, 1649)\n",
      "(413, 1649)\n"
     ]
    }
   ],
   "source": [
    "# if n_components is not set all components are kept: n_components == min(n_samples, n_features)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01878803, 0.01560147, 0.01371565, 0.01320122, 0.01148068,\n",
       "       0.00955322, 0.00838511, 0.00775567, 0.0074108 , 0.00678715,\n",
       "       0.00663369, 0.0064126 , 0.00597942, 0.00568011, 0.00549014,\n",
       "       0.00515012, 0.00466549, 0.00463785, 0.00453808, 0.00449982])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for components/dimensions to cover 95% variance\n",
    "\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    \n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    \n",
    "    # For the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "            \n",
    "    # Return the number of components\n",
    "    return n_components\n",
    "\n",
    "# Run function\n",
    "select_n_components(explained_variance, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9502226944683085\n",
      "1649\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "import numpy as np\n",
    "arr = np.array(explained_variance).tolist()\n",
    "count = 0\n",
    "for i in arr[0:select_n_components(explained_variance, 0.95)]:\n",
    "    count += i\n",
    "print (count)\n",
    "\n",
    "print(explained_variance.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=802)\n",
    "pca.fit(train_x_scaled)\n",
    "\n",
    "train_x_scaled_pca = pca.transform(train_x_scaled)\n",
    "test_x_scaled_pca = pca.transform(test_x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFC with PCA variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eugene\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training the model: 249.20458364486694\n",
      "Best parameters found:\n",
      " {'bootstrap': True, 'max_depth': 35, 'max_features': 5, 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 200}\n",
      "Mean test score: 0.28987197199963155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "rfc = RandomForestClassifier()\n",
    "parameter_space = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [25, 30, 35],\n",
    "    'max_features': [4, 5, 6],\n",
    "    'min_samples_leaf': [3, 5],\n",
    "    'min_samples_split': [10, 12],\n",
    "    'n_estimators': [100, 200],\n",
    "    # 'class_weight': [weights]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(rfc, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(train_x_scaled_pca, train_y)\n",
    "print('Time taken for training the model: '+ str(time.time() - start_time))\n",
    "\n",
    "optimised_rf = clf.best_estimator_\n",
    "\n",
    "# Best parameter set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "print('Mean test score:', max(clf.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred_pca = test_y , optimised_rf.predict(test_x_scaled_pca)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# precision = true positive/(true positive + false positive)\n",
    "# recall = true positive/(true positive + false negative)\n",
    "# f1 score = 2 * (precision * recall)/(precision + recall)\n",
    "\n",
    "#print('Results on the test set:')\n",
    "#print(classification_report(y_true, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_true, y_pred_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# print(confusion_matrix(y_true, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
