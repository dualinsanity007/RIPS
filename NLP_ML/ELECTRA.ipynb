{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bs6blUeGNV7G"
   },
   "outputs": [],
   "source": [
    "!pip install simpletransformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpHey6elPjZE"
   },
   "source": [
    "Run the following as simpletransformers package require Nvidia APEX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiQ06WScNzWj"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/apex\n",
    "!cd apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" /content/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-KmfMR3iN3Ks"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NF1cPOHkPqKp"
   },
   "source": [
    "Importing the cleaned ssoc data and converting the .csv into a pandas frame, with correct labels to be feeded into the model. We removed minor group 336 from the pandas frame as the input data only had 2 observations, which would be inadequate for subsequently spliting. We encourage the user to visualize the .csv before removing limited-observation-minor-groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pMY-TeWN4vk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import io\n",
    "import logging\n",
    "import wandb\n",
    "from simpletransformers.classification import ClassificationArgs, ClassificationModel\n",
    "from statistics import mean, mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ssoc_data = pd.read_csv('ssoctrain_updated.csv',encoding='iso-8859-1')\n",
    "ssoc_data = ssoc_data[[\"E_OCC_Desc\",\"E_OCC\"]]\n",
    "ssoc_data = ssoc_data.rename(columns={'E_OCC_Desc':'text','E_OCC':'labels'})\n",
    "\n",
    "#remove minor group 336 because only 2 entries\n",
    "trunc_indices = ssoc_data.index[ssoc_data['labels'] == 336].tolist()\n",
    "ssoc_data_trunc = ssoc_data.drop(trunc_indices)\n",
    "dummy = dict(enumerate(sorted(ssoc_data_trunc['labels'].unique())))\n",
    "dummy = {value:key for key, value in AB.items()}\n",
    "new_ssoc_data_trunc = ssoc_data_trunc.replace(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axu1d6hOQSbq"
   },
   "source": [
    "We spilt the cleaned data into data for training, evaluation and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNWKM4y8Og9i"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = new_ssoc_data_trunc['labels']\n",
    "x = new_ssoc_data_trunc['text']\n",
    "train_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.2,random_state=1, stratify=y)\n",
    "train1_data = pd.concat([train_x,train_y],axis=1)\n",
    "test_data = pd.concat([test_x,test_y],axis=1)\n",
    "var1 = train1_data['text']\n",
    "var2 = train1_data['labels']\n",
    "train_train_x, eval_train_x, train_train_y, eval_train_y = train_test_split(var1,var2,test_size = 0.25,random_state=1,stratify=var2)\n",
    "train_data = pd.concat([train_train_x, train_train_y],axis=1)\n",
    "eval_data = pd.concat([eval_train_x,eval_train_y],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZDUGNRYQ1De"
   },
   "source": [
    "This is the code to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlL7w6yrOyLp"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Optional model configuration\n",
    "model_args = ClassificationArgs(sliding_window = True)\n",
    "model_args.num_train_epochs = 5\n",
    "model_args.learning_rate = 1e-4\n",
    "model_args.train_batch_size = 64\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.no_save = True\n",
    "###########################################\n",
    "model_args.use_early_stopping = True\n",
    "model_args.early_stopping_delta = 0.01\n",
    "model_args.early_stopping_metric = \"mcc\"\n",
    "model_args.early_stopping_metric_minimize = False\n",
    "model_args.early_stopping_patience = 5\n",
    "model_args.evaluate_during_training_steps = 1000\n",
    "model_args.evaluate_during_training = True\n",
    "###########################################\n",
    "\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    \"electra\",\n",
    "    \"google/electra-base-discriminator\",\n",
    "    # \"bert\",\n",
    "    # \"bert-base-uncased\",\n",
    "    num_labels=66,\n",
    "    #use_cuda = False,\n",
    "    args=model_args,\n",
    ") \n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_data, eval_df=eval_data, accuracy=lambda truth, \n",
    "                  predictions: accuracy_score(truth, [round(p) for p in predictions]))#, output_dir = r'D:\\Academia\\UNI\\RIPS\\ELECTRA Jupyter output')\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_data,verbose = True)#, output_dir = r'D:\\Academia\\UNI\\RIPS\\ELECTRA Jupyter output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OioQmSELRNjZ"
   },
   "source": [
    "We make use of the library ```wandb``` for visualization and finetuning the model. The following cell is to authorize wandb on this notebook. Should you desire to use wandb, you need to create an account on their webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uARVQbdEO504"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otLw8rUMRipG"
   },
   "source": [
    "This code is needed to prevent errors in the subsequent fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYUVleCYO9Bp"
   },
   "outputs": [],
   "source": [
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "try:\n",
    "     torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kw1SVIXqO9p6"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\":\"sweep-test\",\n",
    "    \"method\": \"bayes\",  # grid, random\n",
    "    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"min\":1,\"max\":3},\n",
    "        \"learning_rate\": {\"min\": 0, \"max\": 1e-4},\n",
    "        #\"train_batch_size\":{\"values\":[8,16,32,64,128]}\n",
    "    },\n",
    "    #W&B Sweeps can also speed up the hyperparameter optimization by terminating any poorly performing runs\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 6,}\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"ELECTRA_Hyperparameter_Optimization\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "model_args = ClassificationArgs()\n",
    "model_args.eval_batch_size = 8\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_silent = False\n",
    "model_args.evaluate_during_training_steps = 1000\n",
    "model_args.learning_rate = 4e-4\n",
    "model_args.manual_seed = 4\n",
    "model_args.max_seq_length = 256\n",
    "model_args.multiprocessing_chunksize = 5000\n",
    "model_args.no_cache = True\n",
    "model_args.no_save = True\n",
    "model_args.num_train_epochs = 3\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.train_batch_size = 16\n",
    "model_args.gradient_accumulation_steps = 2\n",
    "model_args.train_custom_parameters_only = False\n",
    "model_args.wandb_project = \"ELECTRA_Hyperparameter_Optimization\"\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init()\n",
    "    print(\"HyperParams=>>\", wandb.config.epochs)\n",
    "\n",
    "    # Create a TransformerModel\n",
    "    model = ClassificationModel(\n",
    "        \"electra\",\n",
    "        \"google/electra-base-discriminator\",\n",
    "        use_cuda = True,\n",
    "        args=model_args,\n",
    "        sweep_config=wandb.config,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.train_model(train_data, eval_df=eval_data, \n",
    "                      accuracy=lambda truth, predictions: accuracy_score(truth, [round(p) for p in predictions]), \n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval_model(test_data,verbose=True)\n",
    "\n",
    "    # Sync wandb\n",
    "    wandb.join()\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJLDvMvARoXy"
   },
   "source": [
    "As Google allocated a limited amount of GPU, we often run into CUDA errors from running multiple ML training. As such, we force restart the notebook with the following 2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mp4KUodCPMTm"
   },
   "outputs": [],
   "source": [
    "!ps -aux|grep python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6UEtkprR1so"
   },
   "source": [
    "Select the 3 digit number which represents the .ipynb launcher. It was 322 previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHiOAPkyPOds"
   },
   "outputs": [],
   "source": [
    "!kill -9 322"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ELECTRA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
